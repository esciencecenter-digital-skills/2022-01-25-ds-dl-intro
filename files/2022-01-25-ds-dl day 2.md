![](https://i.imgur.com/iywjz8s.png)


# 2022-01-25-ds-dl day 2

Welcome to The Workshop Collaborative Document.

This Document is synchronized as you type, so that everyone viewing this page sees the same text. This allows you to collaborate seamlessly on documents.

----------------------------------------------------------------------------

This is the Document for today: [link](https://tinyurl.com/ds-dl-day2)

Collaborative Document day 1: [link](https://tinyurl.com/ds-dl-day1)

Collaborative Document day 2: [link](https://tinyurl.com/ds-dl-day2)

Collaborative Document day 3: [link](https://tinyurl.com/ds-dl-day3)

## üëÆCode of Conduct

* Participants are expected to follow those guidelines:
* Use welcoming and inclusive language.
* Be respectful of different viewpoints and experiences.
* Gracefully accept constructive criticism.
* Focus on what is best for the community.
* Show courtesy and respect towards other community members.
 
## ‚öñÔ∏è License

All content is publicly available under the Creative Commons Attribution License: [creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).

## üôãGetting help

To ask a question, type `/hand` in the chat window.

To get help, type `/help` in the chat window.

You can ask questions in the document or chat window and helpers will try to help you.

## üñ• Workshop website

[link]([<url>](https://esciencecenter-digital-skills.github.io/2022-01-25-ds-dl-intro/))

üõ† Setup

[link](https://esciencecenter-digital-skills.github.io/2022-01-25-ds-dl-intro/#setup)

Download files

[link](https://esciencecenter-digital-skills.github.io/2021-11-22-ds-dl-intro/#downloading-the-required-datasets)

## üë©‚Äçüè´üë©‚Äçüíªüéì Instructors

Sven van der Burg, Djura Smits

## üßë‚Äçüôã Helpers

Cunliang Geng, Robin Richardson  


## üóìÔ∏è Agenda
| | |
|-|-|
|09:00|Welcome and icebreaker|
|09:15|Classification by a Neural Network using Keras|
|10:15|Coffee break|
|10:30|Monitoring the training progress|
|11:45|Tea break|
|12:00|Monitoring the training progress|
|13:00|END|

## üß† Collaborative Notes

***Section: Classification by a Neural Network using Keras***

### Choose a loss function and optimizer
```python=
model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.CategoricalCrossentropy())
```
- [Keras loss functions](https://keras.io/api/losses/)
- [Kears optimizers](https://keras.io/api/optimizers/)

### Train model
```python=
history = model.fit(X_train, y_train, epochs=100)
sns.lineplot(x=history.epoch, y=history.history['loss'])
```

### Perform a prediction/classification
```python=
y_pred = model.predict(X_test)
prediction = pd.DataFrame(y_pred, columns=target.columns)
prediction
```
```python=
predicted_species = prediction.idxmax(axis="columns")
predicted_species
```

### Measuring performance

```python=
from sklearn.metrics import confusion_matrix

true_species = y_test.idxmax(axis="columns")
matrix = confusion_matrix(true_species, predicted_species)
print(matrix)
```

```python=
confusion_df = pd.DataFrame(matrix, index=y_test.columns.values, columns=y_test.columns.values)

confusion_df.index.name = "True label"
confusion_df.columns.name = "Predicted label"

sns.heatmap(confusion_df, annot=True)
```

#### Exercise: Confusion matrix
Measure the performance of the neural network you trained and visualize a confusion matrix.

1. Did the neural network perform well on the test set?
2. Did you expect this from the training loss you saw?
3. What could we do to improve the performance?

### Tune hyperparameters
 will be discussed in day3 
 
### Share your model
save your model
```python=
model.save("my_first_model")
```
load your model for reuse
```python=
pretrained_model = keras.models.load_model("my_first_model")

# use the pretrained model here
y_pretrained_pred = pretrained_model.predict(X_test)
pretrained_prediction = pd.DataFrame(y_pretrained_pred, columns=target.columns.values)

# idxmax will select the column for each row with the highest value
pretrained_predicted_species = pretrained_prediction.idxmax(axis="columns")
print(pretrained_predicted_species)
```
---
***Section: Monitor the training process***

### 1. Import and explore the data

Download the data:
```python=
import pandas as pd
data = pd.read_csv("https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1")
```
#### Exercise: Explore the dataset
Let‚Äôs get a quick idea of the dataset.

1. How many data points do we have?
2. How many features does the data have (don‚Äôt count month and date as a feature)? In total and per city?
3. What are the different measured variable types in the data and how many are there (humidity etc.) ?
 q

| Column 1 | Q1 | Q2 | Q3 |
| -------- | -------- | -------- | -------- |
| Room 1   | 3654     | 89, 11     | cloud_cover, humidity, pressure, global_radiation, precipitation, sunshine, temp_mean, temp_min, temp_max |
| Room 2   | 3654     | 89, TOURS 7 SONNBLICK 8 MUENCHEN 9 MALMO 4 MAASTRICHT 9 KASSEL 8 HEATHROW 9 DUSSELDORF 9 DRESDEN 8 DE_BILT 9 BASEL 9     |cloud_cover,humidity,global_radiation,precipitation,sunshine,temp_mean,temp_min,temp_max     |
| Room 3   | Text     | Text     |Text     |
| Room 4   | 3654     | 89 [per city: 9	9	8	9	9	8	9	4	9	8	7] [per column: 8  10  8  10  11  9  11  11  11]    |cloud_cover',humidity',pressure',global_radiation',precipitation','sunshine'temp_mean',temp_min',temp_max. Some cities have info on winds (but not all of them).

```python=
data.shape

{s.split('_')[-1] for s in data.columns if s not in ['MONTH', 'DATE']}
```

### 2. Define the problem: Predict tomorrow‚Äôs sunshine hours

```python=
nr_rows = 365*3
X_data = data.loc[:nr_rows].drop(columns=["DATE", "MONTH"])
y_data = data.loc[1:(nr_rows + 1)]["BASEL_sunshine"]
```

### 3. Prepare the data for machine learning
#### Check the statistics of the data
```python=
data.describe()
```

#### Clean the data
Look into the name patterns
```python= 
for x in data.columns:
    print(x)
```
where underscores separate the name elements; the first underscore separates the station name from the weather parameters except for the station "DE_BILT".

The Python line below fixes this with a rather compact writing: 

```python=
print({ '_'.join(x.replace("DE_BILT","DE-BILT").split("_")[1:]) for x in data.columns if x not in ["DATE", "MONTH"] }) 
```

In a nutshell, it **replaces** "DE_BILT" with "DE-BILT". Then **splits** each pattern in a `data.columns` line using the underscore delimiter; discards the station name in the first item; and **joins** the remaining elements again.
The {} indicates that we are working with a *set* rather than with a *list*. Therefore, repeated elements are counted only once.
The output is a list of eleven weather parameters that are measured in at least one of the weather stations.

The pythonic oneliner above does pretty much the same as this more readable block:
```python=
weather_features = set()
for a in data.columns:
    if a not in ["DATE", "MONTH"]:
        b = a.replace("DE_BILT", "DE-BILT")
        c = b.split("_")[1:]
        d = '_'.join(c)
        print(a, ' > ', b, ' > ', c, ' > ', d)  # show the logic
        weather_features.add(d)
print(weather_features)
```

#### Prepare training, validation and test datasets
```python=
from sklearn.model_selection import train_test_split

X_train, X_holdout, y_train, y_holdout = train_test_split(X_data, y_data, test_size=0.3, random_state=0)

X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0)
```
#### Exercise: Split data into training, validation and test set 
We have been rather generous at selecting rows from the dataset. Our holdout set above amounts to almost an entire year of data. How would the code need to be rewritten in order to obtain two months of data for the validation and test set each?

1. `X_train, X_holdout ... = train_test_split( ..., test_size = .12, ...)`
    `X_val, X_test ‚Ä¶ = train_test_split( ‚Ä¶, test_size = 2, ‚Ä¶)`
    

2. `X_train, X_holdout ... = train_test_split( ..., test_size = .33, ...)`
    `X_val, X_test ... = train_test_split( ..., test_size = .33, ...)`


3. `X_train, X_holdout ... = train_test_split( ..., test_size = (4./36.), ...)`
    `X_val, X_test ... = train_test_split( ..., test_size = .5, ...)`

4. `X_train, X_holdout ... = train_test_split( ..., test_size = 365, ...)`
    `X_val, X_test ... = train_test_split( ..., test_size = .5, ...)`



### 4. Build a dense neural network


#### Exercise: Design the neural network (breakout)
- What must here be the dimension of our input layer?
- How would our output layer look like? What about the activation function? Tip: Remember that the activation function in our previous classification network scaled the outputs between 0 and 1.


|  | Q1 | Q2 |
| -------- | -------- | -------- |
| Room 1   | (89, 1)    | We would use a linear activation function. Using an activation function that maps everything to a positive value would not penalize large negative values more. Eventually the output value should be between 0 and max(sunshine) of all cities      |
| Room 2   | x_train.shape[1]     | Activation function of output should result in one value between 0 and max(sunshine in basel)     |
| Room 3   | 89     | one value between 0 and 8 (opting for relu)   |
| Room 4   | 89     | 1 value, activation for regression problems: linear activation      |


```python=
from tensorflow.random import set_seed
set_seed(2)
```
```python=
def relu(x):
    return max(0,x)
```
```python=
from tensorflow import keras

def create_nn():
    # Input layer
    inputs = keras.Input(shape=(X_data.shape[1],), name='input')

    # Hidden layers (dense)
    layers_hidden = keras.layers.Dense(100, 'relu')(inputs)
    layers_dense = keras.layers.Dense(50, 'relu')(layers_hidden)
    
   
    # Output layer
    outputs = keras.layers.Dense(1)(layers_dense)

    return keras.Model(inputs=inputs, outputs=outputs, name="weather_prediction_model")

model = create_nn()
    
```
check the model architecture
```python=
model.summary()
```

add loss, optimizer and performance metric
```python=
model.compile(
    loss='mse',
    optimizer='adam', 
    metrics=[keras.metrics.RootMeanSquaredError()]
)
```
- https://keras.io/api/losses/
- https://keras.io/api/optimizers/
- https://keras.io/api/metrics/

### 5. Train the built network

train the model
```python=

history = model.fit(X_train, y_train,
                    batch_size=32,
                    epochs=200,
                    verbose=2)

```

check the performance
```python=
import seaborn as sns
import matplotlib.pyplot as plt


history_df = pd.DataFrame.from_dict(history.history)
sns.lineplot(data=history_df['root_mean_squared_error'])
plt.xlabel("epochs")
plt.ylabel("RMSE")

```
### 6. Evaluate the model

Generates output predictions for the input samples

```python=
y_train_predicted = model.predict(X_train)
y_test_predicted = model.predict(X_test)
```
- https://keras.io/api/models/model_training_apis/#predict-method

```python=
fig, axes = plt.subplots(1, 2, figsize=(12, 6))
plt.style.use('ggplot')  # optional, that's only to define a visual style
axes[0].scatter(y_train_predicted, y_train, s=10, alpha=0.5, color="teal")
axes[0].set_title("training set")
axes[0].set_xlabel("predicted sunshine hours")
axes[0].set_ylabel("true sunshine hours")

axes[1].scatter(y_test_predicted, y_test, s=10, alpha=0.5, color="teal")
axes[1].set_title("test set")
axes[1].set_xlabel("predicted sunshine hours")
axes[1].set_ylabel("true sunshine hours")
```

check the loss value & metrics values for the model in test mode
```python=
loss_train, rmse_train = model.evaluate(X_train, y_train)
loss_test, rmse_test = model.evaluate(X_test, y_test)
```
- https://keras.io/api/models/model_training_apis/#evaluate-method

## üí≠ Feedback
Think about todays lesson, please write (at least) one thing
that went well and (at least) one thing that can be improved.

Think about content (anything you liked or missed), logistics, pace of instructions, instructors/helpers, setup instructions etc.



### What went well:
* explanations are good and on point; really kind and forthcoming teachers +1
* very detailed explanations
* interesting examples and session
* Nice examples, handy to work with
* Nice to have the code also in the shared document 
* Please continue the breakout sessions with exercises, they were nice! :+1:
* +1
### What can be improved:
* the pace could be sped up a little (which probably contradicts with taking the time to have nice and careful explanations) :+4: -1
* better examples to work with +
* It could be helpful if a bit more code comments were included
* I had problems sometimes with the syntax when typing the code. The code in this shared document also contained some typo's. Indeed the note taker may want to test the code before posting.
* Building up the different concepts slowly could help (e.g. first you go for a small NN, then increase size and see differences to explain overfitting); also defining what the expected "good" behavior is (the baseline error zeg maar). 
* I am sometimes busy with writing rather the listening to the explanation :+2: 
* Often I'm searching for what part of the deep learning workflow we are at at times (are we cleaning data, splitting test and trian, making hidden or input layers etc.) having some overview of the whole process and indicating where we are in the process on that visual aid would really help. This also feeds into the point made 1 bullet above :+1: 
* for post-covid times, always would be nice to have such workshops live
* I think it would be nice if you could direct more to further reading or give some suggestions for reading outside the course, so that you can focus on the practical side. I do like the discussions about the theory but having a clear understandable source would be more effective in my opinion.   :+1: 
* have some PPT slides for frequently questions(especially the terms which are difficult to describe only by words).for instance, what is an epoch, what is backpropagation? :+1: 

## üìö Resources
- [what is loss function](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)
- [a step-by-step backpropagation example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
- [epoch, batch and iteration](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9); note that 'epoch' in general computing (Python included) can also indicate [the starting point you compute time from](https://pythonguides.com/python-epoch-to-datetime/) 
-  libraries of pretrained models:
    -  https://modelzoo.co/
    -  https://keras.io/api/applications/
    -  https://pytorch.org/hub/
    -  https://www.tensorflow.org/hub/
- miscellaneous wikipedia links:
    - https://en.wikipedia.org/wiki/Backpropagation
    - https://en.wikipedia.org/wiki/Data_dredging
- Tensorflow playground (visual demo of training a deep learning model used on day 1) https://playground.tensorflow.org/

- Probabilistic Bayesian Neural Networks
https://keras.io/examples/keras_recipes/bayesian_neural_networks/
